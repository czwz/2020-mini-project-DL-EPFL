import torch
import math

from torch import optim
from torch import Tensor
from torch import nn
from torch.nn import functional as F

import dlc_practical_prologue as prologue


# ------------------------------------------------------------------------------------------------------------
# Helper functions
def data_preparation():
    '''
    This function prepares 1000 pairs of training data testing data respectively. 
    
    Return:
    	train: a dictionary of keys 'input', 'classes', 'target'
    	test:  a dictionary of keys 'input', 'classes', 'target'
    '''
    train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(1000)
    train, test = {}, {}
    
    train['input'], test['input'] = train_input, test_input
    train['classes'], test['classes'] = train_classes, test_classes
    train['target'], test['target'] = train_target, test_target
    
    # Normlisation
    mean, std = train['input'].mean(), train['input'].std()
    train['input'] = train['input'].sub_(mean).div_(std)
    test['input'] = test['input'].sub_(mean).div_(std)
    
    return train, test


def compute_nb_errors(model, test, mini_batch_size=10):
    '''
    Calculates the number of errors and the error rate when applying a model on the test dataset. 
    
    Params:
        model: the model to be evaluated
        test:  a dictionary generated by the function "data_preparation"
        
    Return: 
        the number of errors
        the error rate
    '''
    
    nb_errors = 0
    test_input, test_target = test['input'], test['target']
    
    for b in range(0, test_input.size(0), mini_batch_size):
        output, _, _= model(test_input.narrow(0, b, mini_batch_size))
        
        if output.dim() == 1:
            predicted_target = output
        else:
            _, predicted_target = torch.max(output, 1)
            
        for k in range(mini_batch_size):
            if test_target[b + k] != predicted_target[k]:
                nb_errors += 1
                
    return nb_errors, nb_errors / test['target'].shape[0]


def train_and_test(rounds, Net_name, training_function):
    '''
    Evaluate the constructed model and return the performance over given iterations. 
    
    Params:
        rounds: Total number of iterations
        Net_name: the class name of the network that is to be evaluated
        training_function: the corresponding training function that is used to train the model 
        
    Returns:
        expectation: the average error rate
        std_var: the standard variation of the error rate
    '''
    error_rates = []
    for i in range(rounds):
        print("Iteration", i)
        
        # Generate new data for this iteration
        train, test = data_preparation()
        
        # Create, train and test the model
        model = Net_name(200)
        training_function(model, train)
        
        nb_errors, error_rate = compute_nb_errors(model, train)
        print("train error: ", error_rate * 100, "%")
        nb_errors, error_rate = compute_nb_errors(model, test)
        print("test error: ", error_rate * 100, "%")
        error_rates.append(error_rate)
    
    error_rates = Tensor(error_rates)
    expectation, std_var = torch.mean(error_rates), torch.std(error_rates)
    print('The average error rate is {}%'.format(expectation.item() * 100)), 
    print('The standard deviation of error rate is {}%'.format(std_var.item()*100))
    return expectation, std_var

# ------------------------------------------------------------------------------------------------------------
# Model 1:
print('Model 1 (Baseline): a CNN with input as a two-channle image pair')

class CNN_baseline(nn.Module):
    def __init__(self, nb_hidden):
        super(CNN_baseline, self).__init__()
        # The input to the CNN is a single two-channel image
        self.conv1 = nn.Conv2d(2, 32, kernel_size=3)  # 32 * 12 * 12 --maxPool> 32 * 6 * 6
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3) # 64 * 4  * 4  --maxPool> 64 * 2 * 2 = 256
        self.fc1 = nn.Linear(256, nb_hidden)
        self.fc2 = nn.Linear(nb_hidden, 2)
        # The number of params:
        # conv1: 32 * (2 * 3 * 3 + 1) = 608
        # conv2: 64 * (32 * 3 * 3 + 1) = 18,496
        # fc1: (256 + 1) * 200 = 51,400
        # fc2: (200 + 1) * 2 = 402

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2))
        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2))
        x = F.relu(self.fc1(x.view(-1, 256)))
        x = self.fc2(x)
        return x, None, None

def train_model_baseline(model, train, mini_batch_size=10, EPOCHS=25, LR=1e-2):
    '''
    Trains the baseline CNN model. 
    
    Params:
        model: a baseline CNN model
        train: a dictionary generated by the function "data_preparation"
    '''
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr = LR)
    
    train_input, train_target = train['input'], train['target']
    for e in range(EPOCHS):
        sum_loss = 0
        for b in range(0, train_input.size(0), mini_batch_size):
            output, _, _ = model(train_input.narrow(0, b, mini_batch_size))
            loss = criterion(output, train_target.narrow(0, b, mini_batch_size))
            model.zero_grad()
            loss.backward()
            sum_loss = sum_loss + loss.item()
            optimizer.step()

expecation, std_var = train_and_test(15, CNN_baseline, train_model_baseline)
print("---------------------------------------------------------------------------")


# ------------------------------------------------------------------------------------------------------------
# Model 2:
print('Model 2: Two CNNs without weight-sharing')

class CNN_wo_sharing(nn.Module):
    def __init__(self, nb_hidden):
        super(CNN_wo_sharing, self).__init__()
        
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)
        self.fc1 = nn.Linear(256, nb_hidden)
        self.fc2 = nn.Linear(nb_hidden, 10)
        
        self.conv1_ = nn.Conv2d(1, 32, kernel_size=3)
        self.conv2_ = nn.Conv2d(32, 64, kernel_size=3)
        self.fc1_ = nn.Linear(256, nb_hidden)
        self.fc2_ = nn.Linear(nb_hidden, 10)
        
        self.fc3 = nn.Linear(20, 200)
        self.fc4 = nn.Linear(200, 2)

    def aux_left(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2))
        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2))
        x = F.relu(self.fc1(x.view(-1, 256)))
        x = self.fc2(x)
        return x
    
    def aux_right(self, x):
        x = F.relu(F.max_pool2d(self.conv1_(x), kernel_size=2))
        x = F.relu(F.max_pool2d(self.conv2_(x), kernel_size=2))
        x = F.relu(self.fc1_(x.view(-1, 256)))
        x = self.fc2_(x)
        return x
    
    def forward(self, x):
        imgs_left, imgs_right = x.narrow(1,0,1), x.narrow(1,1,1)
        output_left, output_right = self.aux_left(imgs_left), self.aux_right(imgs_right) 
        
        x = torch.cat([output_left,output_right],dim=1)
        x = F.relu(self.fc3(x))
        output = self.fc4(x)
        return output, output_left, output_right
    
    
def train_model(model, train, aux_loss=True, mini_batch_size=10, EPOCH=25, LR=15e-2):
    '''
    Trains the neural network for model 2 and model 3
    
    Params:
        model: a baseline CNN model
        train: a dictionary generated by the function "data_preparation"
    '''
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr = LR)
    
    for e in range(EPOCH):
        epoch_loss = 0
        for b in range(0, train['input'].size(0), mini_batch_size):
            
            img_pairs, num_pairs, targets = train['input'].narrow(0, b, mini_batch_size), train['classes'].narrow(0, b, mini_batch_size), train['target'].narrow(0, b, mini_batch_size)
            num_left, num_right = num_pairs[:, 0], num_pairs[:, 1]
            
            result, output_left, output_right = model(img_pairs)
            
            # Compute loss
            if aux_loss:
                final_loss = criterion(result, targets)
                left_loss, right_loss = criterion(output_left, num_left), criterion(output_right, num_right)
                loss = 0.4*final_loss + 0.3*left_loss + 0.3*right_loss
                epoch_loss += loss
            else:
                loss = criterion(output, targets)
                epoch_loss += loss
            
            model.zero_grad()
            loss.backward()
            optimizer.step()

expecation, std_var = train_and_test(15, CNN_wo_sharing, train_model)
print("---------------------------------------------------------------------------")



# ------------------------------------------------------------------------------------------------------------
# Model 3:
print('Model 3: Siamese CNN with fully connected layers for decision')

class CNN_Siamese_fc(nn.Module):
    def __init__(self, nb_hidden):
        super(CNN_Siamese_fc, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)
        self.fc1 = nn.Linear(256, nb_hidden)
        self.fc2 = nn.Linear(nb_hidden, 10)
        
        self.fc3 = nn.Linear(20, 200)
        self.fc4 = nn.Linear(200, 2)
        # The number of params:
        # conv1: 32 * (2 * 3 * 3 + 1) = 608
        # conv2: 64 * (32 * 3 * 3 + 1) = 18,496
        # fc1: (256 + 1) * 200 = 51,400
        # fc2: (200 + 1) * 2 = 402

    def aux_forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2))
        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2))
        x = F.relu(self.fc1(x.view(-1, 256)))
        x = self.fc2(x)
        return x
    
    def forward(self, x):
        imgs_left, imgs_right = x.narrow(1,0,1), x.narrow(1,1,1)
        output_left, output_right = self.aux_forward(imgs_left), self.aux_forward(imgs_right) 
        
        x = torch.cat([output_left,output_right],dim=1)
        
        x = F.relu(self.fc3(x))
        x = self.fc4(x)
        
        return x, output_left, output_right

expecation, std_var = train_and_test(15, CNN_Siamese_fc, train_model)
print("---------------------------------------------------------------------------")





# ------------------------------------------------------------------------------------------------------------
# Model 4:
print('Model 4: Siamese CNN with manual logic for decision')

class CNN_Siamese_logical(nn.Module):
    def __init__(self, nb_hidden):
        super(CNN_Siamese_logical, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)
        self.fc1 = nn.Linear(256, nb_hidden)
        self.fc2 = nn.Linear(nb_hidden, 10)

    def aux_forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2))
        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2))
        x = F.relu(self.fc1(x.view(-1, 256)))
        x = self.fc2(x)
        return x
    
    def forward(self, x):
        imgs_left, imgs_right = x.narrow(1,0,1), x.narrow(1,1,1)
        output_left, output_right = self.aux_forward(imgs_left), self.aux_forward(imgs_right) 
        _, nums_left = torch.max(output_left, 1)
        _, nums_right = torch.max(output_right, 1)
        result = (nums_left <= nums_right).int()
        return result, output_left, output_right

def train_CNN_logical(model, train, aux_loss=True, mini_batch_size=10, EPOCH=25, LR=8e-2):
    '''
    Trains the neural network for model 4
    
    Params:
        model: a baseline CNN model
        train: a dictionary generated by the function "data_preparation"
    '''
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr = LR)
    
    for e in range(EPOCH):
        epoch_loss = 0
        for b in range(0, train['input'].size(0), mini_batch_size):
            img_pairs, num_pairs, targets = train['input'].narrow(0, b, mini_batch_size), train['classes'].narrow(0, b, mini_batch_size), train['target'].narrow(0, b, mini_batch_size)
            num_left, num_right = num_pairs[:, 0], num_pairs[:, 1]
            
            result, output_left, output_right = model(img_pairs)
            
            left_loss, right_loss = criterion(output_left, num_left), criterion(output_right, num_right)
            loss = 0.5*left_loss + 0.5*right_loss
        
            epoch_loss += loss
            
            model.zero_grad()
            loss.backward()
            optimizer.step()

            
expecation, std_var = train_and_test(15, CNN_Siamese_logical, train_CNN_logical)
print("The END")



